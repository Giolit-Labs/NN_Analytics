Changes:
    Activation functions have been changed in the activation.h and derivative.h file.
    Changed the learning rate to .0000002.
    Changed the data set.
    Each layer have been used with different activation functions:
        Layer 1: Sigmoid
        Layer 2: Relu
        Layer 3: Leaky_Relu
    3 neurons in each layer with custom weight initialization.
